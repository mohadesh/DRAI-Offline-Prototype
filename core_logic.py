# core_logic.py
import os
import sys
import uuid
import shutil
import logging
import pandas as pd
import subprocess
from pathlib import Path

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

BASE_DIR = Path(__file__).resolve().parent
UPLOADS_DIR = BASE_DIR / "uploads"
OUTPUT_DIR = BASE_DIR / "outputs"  # Ù¾ÙˆØ´Ù‡ Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÛŒØ§Ù†ÛŒ

# Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§
UPLOADS_DIR.mkdir(exist_ok=True)
OUTPUT_DIR.mkdir(exist_ok=True)

def run_script(script_name, args):
    """
    Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ© Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù¾Ø§ÛŒØªÙˆÙ† Ø¯ÛŒÚ¯Ø± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡.
    Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø²Ù†Ø¯Ù‡ (Real-time) Ø¯Ø± ØªØ±Ù…ÛŒÙ†Ø§Ù„ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    script_path = BASE_DIR / script_name
    if not script_path.exists():
        raise FileNotFoundError(f"Script not found: {script_path}")

    command = [sys.executable, str(script_path)] + args
    
    logger.info(f"ğŸš€ Running {script_name} with args: {args}")
    
    try:
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Popen Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø²Ù†Ø¯Ù‡
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1,
            universal_newlines=True,
            encoding='utf-8', 
            errors='replace' # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ú©Ø±Ø´ Ú©Ø±Ø¯Ù† Ø±ÙˆÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ ÙØ§Ø±Ø³ÛŒ
        )

        # Ø®ÙˆØ§Ù†Ø¯Ù† Ùˆ Ú†Ø§Ù¾ Ø®Ø· Ø¨Ù‡ Ø®Ø· Ø®Ø±ÙˆØ¬ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                print(f"[{script_name}] {output.strip()}")

        # Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ
        stderr_output = process.stderr.read()
        if stderr_output:
            print(f"[{script_name} ERROR] {stderr_output.strip()}")

        if process.returncode != 0:
            raise RuntimeError(f"{script_name} failed with return code {process.returncode}")

        logger.info(f"âœ… {script_name} completed successfully.")

    except Exception as e:
        logger.error(f"âŒ Error running {script_name}: {e}")
        raise

def save_uploaded_files(files, folder):
    """Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒØ³Øª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡ Ø¯Ø± ÛŒÚ© Ù¾ÙˆØ´Ù‡ Ø®Ø§Øµ"""
    saved_paths = []
    for file in files:
        if file:
            file_path = folder / file.filename
            file.save(file_path)
            saved_paths.append(str(file_path))
    return saved_paths

def load_model(path):
    """Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„ (Ø¨Ù‡ ØµÙˆØ±Øª Placeholder Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ù†Ø¨ÙˆØ¯)"""
    import pickle
    try:
        with open(path, "rb") as f:
            return pickle.load(f)
    except Exception as e:
        logger.error(f"Failed to load model from {path}: {e}")
        return None

def run_inference_for_md_c(model_md, model_c, df_window):
    """
    Ø§Ø¬Ø±Ø§ÛŒ Ø§ÛŒÙ†ÙØ±Ù†Ø³ Ø±ÙˆÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Darts.
    Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ df_window Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª.
    """
    try:
        # ØªØ¨Ø¯ÛŒÙ„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ù‡ TimeSeries (Ù…Ø®ØªØµ Darts)
        from darts import TimeSeries
        
        # Ù†Ú©ØªÙ‡: Ù†Ø§Ù… Ø³ØªÙˆÙ† Ø²Ù…Ø§Ù† Ø¨Ø§ÛŒØ¯ Ø¯Ù‚ÛŒÙ‚ Ø¨Ø§Ø´Ø¯ØŒ ÙØ±Ø¶ Ø¨Ø± 'date' ÛŒØ§ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø§Ø³Øª
        # Ø§Ú¯Ø± Ø§ÛŒÙ†Ø¯Ú©Ø³ datetime Ø§Ø³Øª:
        series = TimeSeries.from_dataframe(df_window)

        pred_md = model_md.predict(n=1, series=series)
        pred_c = model_c.predict(n=1, series=series)

        return {
            "MD": pred_md.values()[0][0],
            "C": pred_c.values()[0][0]
        }
    except Exception as e:
        logger.error(f"Inference Error: {e}")
        return None

def process_data(process_files, pellet_files, md_files, model_md_path=None, model_c_path=None):
    """
    ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ú©Ù‡ ØªÙˆØ³Ø· app.py ØµØ¯Ø§ Ø²Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
    ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ùˆ Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§ Ø±Ø§ Ø²Ù†Ø¬ÛŒØ±Ù‡â€ŒØ§ÛŒ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    session_id = str(uuid.uuid4())
    session_dir = UPLOADS_DIR / session_id
    session_dir.mkdir(exist_ok=True)
    
    logger.info(f"ğŸ Starting processing for Session: {session_id}")

    try:
        # 1. Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ù…ÙˆÙ‚Øª
        process_paths = save_uploaded_files(process_files, session_dir)
        pellet_paths = save_uploaded_files(pellet_files, session_dir)
        md_paths = save_uploaded_files(md_files, session_dir)

        # ØªØ¹Ø±ÛŒÙ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ù…ÛŒØ§Ù†ÛŒ
        output_process = session_dir / "Process_Cleaned.csv"
        output_pellet = session_dir / "Pellet_Cleaned.csv"
        output_md = session_dir / "MD_Cleaned.csv"
        output_merged = session_dir / "Merged_Final.csv"

        # ---------------------------------------------------------
        # STEP 1: ProcessTags.py
        # ---------------------------------------------------------
        # Ù†Ú©ØªÙ‡: ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ÙØ§ÛŒÙ„ Ø§ÙˆÙ„ Ù¾Ø±ÙˆØ³Ø³ ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ Ø§Ø³Øª
        run_script("ProcessTags.py", [
            "--input", process_paths[0],
            "--output", str(output_process),
            "--resample-rate", "30T"
        ])

        # ---------------------------------------------------------
        # STEP 2: Pellet.py
        # ---------------------------------------------------------
        # ÙØ±Ø¶: Ù¾Ù„Øª Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ú†Ù†Ø¯ ÙØ§ÛŒÙ„ Ø¨Ø§Ø´Ø¯ ÛŒØ§ ÛŒÚ©ÛŒ. ÙØ¹Ù„Ø§ Ø§ÙˆÙ„ÛŒ Ø±Ø§ Ù¾Ø§Ø³ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
        run_script("Pellet.py", [
            "--input", pellet_paths[0],
            "--output", str(output_pellet)
        ])

        # ---------------------------------------------------------
        # STEP 3: MDnC.py
        # ---------------------------------------------------------
        run_script("MDnC.py", [
            "--input", md_paths[0],
            "--output", str(output_md)
        ])

        # ---------------------------------------------------------
        # STEP 4: Merging (merging.py)
        # ---------------------------------------------------------
        run_script("merging.py", [
            "--process", str(output_process),
            "--pellet", str(output_pellet),
            "--md", str(output_md),
            "--output", str(output_merged)
        ])

        # ---------------------------------------------------------
        # STEP 5: Load Result & Return
        # ---------------------------------------------------------
        if output_merged.exists():
            final_df = pd.read_csv(output_merged)
            
            # Ensure georgian_datetime is proper datetime
            if 'georgian_datetime' in final_df.columns:
                final_df['georgian_datetime'] = pd.to_datetime(final_df['georgian_datetime'])
            elif 'date' in final_df.columns:
                final_df = final_df.rename(columns={'date': 'georgian_datetime'})
                final_df['georgian_datetime'] = pd.to_datetime(final_df['georgian_datetime'])
            elif 'Date' in final_df.columns:
                final_df = final_df.rename(columns={'Date': 'georgian_datetime'})
                final_df['georgian_datetime'] = pd.to_datetime(final_df['georgian_datetime'])
            
            # Drop rows where all data columns are NaN (empty time slots)
            data_cols = [c for c in final_df.columns if c.startswith(('INST_', 'PELLET_', 'MDNC_'))]
            if data_cols:
                final_df = final_df.dropna(subset=data_cols, how='all')
            
            logger.info(f"ğŸ‰ All steps completed. Final shape: {final_df.shape}")
            
            return {
                "success": True,
                "merged_df": final_df,
                "stats": {
                    "rows": len(final_df),
                    "columns": list(final_df.columns)
                }
            }
        else:
            raise FileNotFoundError("Merged file was not created. Check script outputs above.")

    except Exception as e:
        logger.error(f"ğŸ’¥ Critical Error in pipeline: {e}")
        raise e
    
    finally:
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª (Ø§Ø®ØªÛŒØ§Ø±ÛŒ - Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯ ÙØ¹Ù„Ø§ Ú©Ø§Ù…Ù†Øª Ø´Ø¯Ù‡)
        # shutil.rmtree(session_dir, ignore_errors=True)
        pass
